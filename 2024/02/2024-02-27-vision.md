# Vision for Software and Computing in MX

By implication these comments are focused around the needs of MX on Diamond II, but the comments are relatively universal and apply to many facilities in MX

## Introduction

Following the arrival of alphafold2 the drive to develop MX beamlines for phasing has substantially evaporated. There are still cases for phasing beamlines, for example case where predictions are unhelpful, or for metal identification, but these have now been overtaken by other experimental modalities for example fragment screening and time-resolved / perturbation based methods. This change to a large extent represents a fork in the road: the drives for the two science areas and the experimental demands they present seem almost orthogonal, beyond the common push towards massively multi-crystal data sets and the “big data” challenges they bring. Fragment screening demands an obsessive focus on optimising throughput and reliability, while more experimental methods require greater flexibility and adaptability. While these demands to be in conflict, they share a core requirement to fully understand the science goal while having a detailed insight into the behaviour of the beamline instrumentation to deliver that science capability.

## Background: developing automation

The US protein structure initiative (2000-2015) drove the development of many of the components of a modern MX beamline. In Europe this was complimented by the “autostruct” and “biostruct-X” along with SPINE (structural proteomics in Europe) projects. Together these pushed the development of robotic sample changers, standards for sample handling and software for the automation of data collection on MX beamlines. The MX beamlines for phases 1 and 2 of Diamond were built in this period, and focused on automation from the outset. The development of photon counting pixel array detectors (PILATUS, then EIGER) allowed the throughput of MX beamlines to improve by close to an order of magnitude in terms of data collection time, though the overall cycle time has not increased by anywhere near as much. To a large extent many of the components of a modern are available off-the-shelf from a small number of vendors, leading to homogeneity in beamlines and their capabilities. Typically MX beamlines offer a relatively standardised GUI for interactive use (e.g. GDA, blu-ice, mxcube) with the ability to change samples and collect data with very little attention to the details. Collectively MX beamlines have been optimised for an efficient interactive end-user experience, either remotely or locally, however this is proving to be something of an evolutionary dead end.

## Target 1: throughput

Delivering high throughput principally depends on having an efficient pipeline with as few as possible delays: in manufacturing terms this was recognised as the development of the production line, where the focus is on concurrency. This is largely in conflict with a flexible interactive user experience where the user has freedom to select the next course of action: typically the system will have a small number of known-safe states that are returned to at the end of each operation. The optimisation which brought us here requires us to now take a step back and reconsider the architecture of the systems as a whole, defining those actions which can be performed in parallel to reduce the dead time, and between one acquisition and the next whilst retaining the resilience which is necessary to maintain throughput.

The current typical workflow can be thought of as a state machine, with a number of resting or active states connected by transitions

safe for user access
resting
sample exchange
sample alignment
data collection which internally consists of arm detector / acquisition system, configure scan, perform scan, disarm

A fully automated system will know what step will follow the current activity (i.e. which sample is to be loaded next, what is the next scan, how many scans will be performed) which allows greater optimisation for example arming the detector during the sample exchange. This requires a view of the whole system and the high level goal of the experiment, optimising the behaviour of the whole system to deliver the experimental results. A similar approach can be taken to handling the detector data: we know what is coming next so we can optimise the system to reduce delays, e.g. from waiting for the results of a raster scan for X-ray centring a sample. This system view requires working more closely with the detector by pulling data out through a stream rather than fetching completed data at the end of the scan, but allows the results to be available very shortly after the end of the scan rather than waiting for the data to be recovered and analysed.

These considerations raise the need for the detailed science knowledge and instrumentation capabilities to be integral to the development of the software to support the system. Today Python has become the “lingua Franca” of software development in science, and tools written in Python such as bluesky and ophyd enable scientists to work much more closely with support groups or engineers to develop beamline systems, whereas in the past the need to use e.g. C++ or Java to do such systems presented a substantial barrier to collaboration.

It is useful to note that the vision of automated and unattended data collection is not in conflict with interactive use but the medium of interaction changes. In place of an imperative system where the user instructs the system to transition between states, instead the system is capable of pulling instructions from a database, and the source of those instructions could be an automated system or an interactive user interacting over the web.

## Target 2: experimental

The second fork is towards e.g. time resolved methods which require perturbing the sample and recording the response, and may involve integration of bespoke hardware into the beamline environment. This equipment may be standard, but could also be developed and brought by the user. At first glance this requirement may appear to conflict with the desire for a tightly integrated system optimised for high throughput, from a system perspective these have much in common, understanding and choreographing the behaviour of each component in the system to achieve the experimental goal. Once again the existence of Python as a common language for scientific software development comes to our rescue, as it is reasonable to expect most bespoke experiments to be run for example from a Jupyter notebook with custom hardware integrated through user-defined Python scripts. the actual experiment can leverage the integrated tools to support high throughput experiments to give real-time feedback to help optimise the use of experiment time.

## Integration

Bluesky for automation <—> database <—> web UI
Ophyd
Beamline hardware (e.g. mediated by EPICS) <—> data systems

vs.

Bluesky scripts with Jupyter notebook <—> custom hardware
Ophyd
Beamline hardware (e.g. mediated by EPICS) <—> data systems

The conflicting goals presented earlier have much in common - the need for understanding the system as a whole combined with a scientist-friendly software development environment which allows active collaboration between users, beamline staff and support engineers. This reflects the democratisation of modern computing and the desire for more flexible approaches for developing new experiment modalities.

## Action Plan

Most of the work required to deliver the vision is non-technical - understanding the goals of the experiment and the components of the system. Exposing the capabilities of these components in a flexible way and making the software to deliver on experiment goals is work to be performed as a collaboration between the scientists and engineers with a common goal. In many cases the requirements will evolve as the capabilities and limitations of the hardware become apparent, and will evolve further as hardware changes. Adopting a minimal set of tools as a community, such as Python / bluesky will allow some of the common “boring” work to be shared, ensuring that effort is not duplicated and instead can be focused on delivering on the unique capabilities of the beamlines. The emphasis on a flexible set of tools rather than a universal application which will do everything for everyone will ensure that we can continue to adapt to the next set of challenges. The MX community has a long history of open collaboration with a focus on getting science done - a flexible, informal and community focused approach to developing the software and computing will help us to continue this in future.
